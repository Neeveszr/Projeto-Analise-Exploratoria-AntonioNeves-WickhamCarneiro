# ğŸ“¦ Projeto de AnÃ¡lise ExploratÃ³ria - Olist E-Commerce
Impulsionado pela facilidade de compra e diversidade de produtos, o comÃ©rcio eletrÃ´nico brasileiro estÃ¡ bastante presente no dia a dia da populaÃ§Ã£o, em contrapartida, apresenta vÃ¡rios desafios logÃ­sticos significativos, como atrasos na entrega, inconsistÃªncias nos cadastros e grande variaÃ§Ã£o de caracterÃ­sticas de produtos. Essas imperfeiÃ§Ãµes sÃ£o tÃ­picas de um ambiente real de comercio online.


Diante desse cenÃ¡rio, surge o problema real tratado nesse trabalho: O que afeta a experiÃªncia e satisfaÃ§Ã£o do cliente no e-commerce brasileiro?

## ğŸ‘¥ Integrantes
- AntÃ´nio Neves Aguiar Neto
- Wickham Carneiro Pereira

---

## ğŸ”— Base de Dados Utilizada
O projeto utiliza parte do conjunto de dados pÃºblicos da Olist, disponibilizado originalmente no Kaggle:

**Link da base completa:**
[Olist Brazilian E-Commerce Dataset (Kaggle)](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/data)

Datasets utilizados no projeto:
- `olist_orders_dataset.csv`  
- `olist_order_items_dataset.csv`  
- `olist_products_dataset.csv`

Olist_order_items_dataset â€“ Itens vendidos em cada pedido
Olist_orders_dataset â€“ InformaÃ§Ãµes dos pedidos
Olist_products_dataset â€“ InformaÃ§Ãµes dos produtos

---

## ğŸ¯ Objetivo do Projeto
Aplicar o ciclo de vida da CiÃªncia de Dados para descobrir o que afeta a experiÃªncia e satisfaÃ§Ã£o do cliente no e-commerce brasileiro, investigando os padrÃµes que influenciam:

- Atrasos de entrega
- Baixa ou alta satisfaÃ§Ã£o
- DiferenÃ§as de preÃ§o e frete
- Categorias de produtos problemÃ¡ticas
- VariaÃ§Ãµes no tempo de processamento e envio


O objetivo final foi construir um dataset limpo, padronizado e enriquecido, capaz de gerar insights relevantes sobre desempenho logÃ­stico e experiÃªncia do cliente.

---

## ğŸ› ï¸ DescriÃ§Ã£o do Processo de Tratamento dos Dados

O prÃ©-processamento dos dados foi realizado seguindo uma sequÃªncia estruturada de etapas para garantir consistÃªncia, qualidade e confiabilidade das anÃ¡lises. As principais fases foram:

### **1. Carregamento e ExploraÃ§Ã£o Inicial**
Iniciamos com a inspeÃ§Ã£o das bases utilizando `df.head()`, `df.info()` e `df.describe()`.  
Essa etapa permitiu identificar tipos de dados, presenÃ§a de valores ausentes, estatÃ­sticas descritivas e possÃ­veis problemas iniciais.

### **2. IdentificaÃ§Ã£o dos Valores Ausentes**
Com o comando `df.isnull().sum()`, contabilizamos a quantidade de valores ausentes em cada coluna.  
Isso permitiu definir quais atributos precisavam de imputaÃ§Ã£o ou tratamento especÃ­fico.

### **3. Tratamento dos Valores Ausentes**
- VariÃ¡veis numÃ©ricas receberam **imputaÃ§Ã£o pela mediana por categoria**, garantindo coerÃªncia com o comportamento dos produtos.  
- Colunas categÃ³ricas tiveram seus valores ausentes substituÃ­dos por `"desconhecido"`.

### **4. CorreÃ§Ã£o de InconsistÃªncias**
- Valores impossÃ­veis, como **pesos igual a zero ou negativos**, foram substituÃ­dos pela mediana da respectiva categoria de produto.  
- Realizamos padronizaÃ§Ã£o textual (`str.lower()` + `str.strip()`) para evitar categorias duplicadas devido a diferenÃ§as de maiÃºsculas/minÃºsculas ou espaÃ§os extras.

### **5. CodificaÃ§Ã£o das VariÃ¡veis CategÃ³ricas**
Aplicamos **One-Hot Encoding** (`pd.get_dummies`) na coluna `order_status`, permitindo que os modelos e anÃ¡lises futuras trabalhem com variÃ¡veis categÃ³ricas de forma numÃ©rica e interpretÃ¡vel.

### **6. NormalizaÃ§Ã£o / PadronizaÃ§Ã£o das VariÃ¡veis NumÃ©ricas**
Utilizamos o **StandardScaler** para padronizar atributos numÃ©ricos, garantindo escalas equivalentes entre variÃ¡veis e evitando distorÃ§Ãµes em anÃ¡lises que dependem de magnitude.

### **7. CriaÃ§Ã£o de Novas Features (Feature Engineering)**
Foram criados atributos que enriquecem a anÃ¡lise logÃ­stica:
- **Tempo de entrega (delivery_delay_days)** 
- **Atraso binÃ¡rio (is_late_delivery)**
- **Dias de processamento (delivery_time_days)**
- **Custo do frete por peso (freight_per_kg)**

Essas features permitiram compreender melhor o comportamento logÃ­stico e identificar relaÃ§Ãµes nÃ£o visÃ­veis nas colunas originais.

---

## ğŸš§ Principais Desafios Encontrados
- Alta ausÃªncia de dados no dataset de produtos  
- Peso e medidas com valores inconsistentes  
- VariÃ¡veis de data no formato texto  
- Necessidade de integrar trÃªs bases diferentes via merge  
- Outliers legÃ­timos que nÃ£o podiam ser removidos  
- DiferenÃ§as de categoria causadas por erros de escrita

---

## ğŸ“Œ Principais ConclusÃµes 

Foi revelado padrÃµes importantes, as conclusÃµes principais foram:

- **Atrasos dependem tanto do vendedor quanto da transportadora**  
   O tempo que o vendedor leva para processar e despachar o pedido tem forte impacto no atraso final, mostrando que a responsabilidade logÃ­stica Ã© compartilhada.

- **Peso e dimensÃµes influenciam diretamente o valor do frete**  
   Produtos pesados ou volumosos, especialmente mÃ³veis, eletrodomÃ©sticos e utilidades domÃ©sticas grandes, apresentam fretes mais altos devido Ã  complexidade do transporte.

- **A maioria dos pedidos Ã© entregue no prazo ou antes**  
   A distribuiÃ§Ã£o do atraso mostra uma operaÃ§Ã£o estÃ¡vel, com poucas entregas realmente atrasadas.

- **Os dados apresentavam inconsistÃªncias relevantes**  
   Foram encontrados pesos invÃ¡lidos, textos despadronizados e muitos valores ausentes no dataset de produtos. O prÃ©-processamento foi essencial para corrigir esses problemas e garantir anÃ¡lises consistentes.

- **As novas features ampliaram a compreensÃ£o logÃ­stica**  
   As variÃ¡veis criadas (`delivery_delay_days`, `is_late_delivery`, `processing_time_days`, `freight_per_kg`) ajudaram a explicar padrÃµes de atraso, eficiÃªncia de vendedores e comportamento do frete de forma mais clara do que os dados originais permitiam.


---

